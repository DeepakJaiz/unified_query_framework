{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3288b9b5",
   "metadata": {},
   "source": [
    "##### First of all, we will analyze Wikipedia articles of different cities: Boston, Seattle, San Francisco, and more.\n",
    "\n",
    "##### The below code snippet downloads the relevant data into files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a0ce195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "wiki_titles = [\"Toronto\", \"Seattle\", \"Chicago\", \"Boston\", \"Houston\"]\n",
    "\n",
    "for title in wiki_titles:\n",
    "    response = requests.get(\n",
    "        'https://en.wikipedia.org/w/api.php',\n",
    "        params={\n",
    "            'action': 'query',\n",
    "            'format': 'json',\n",
    "            'titles': title,\n",
    "            'prop': 'extracts',\n",
    "            # 'exintro': True,\n",
    "            'explaintext': True,\n",
    "        }\n",
    "    ).json()\n",
    "    page = next(iter(response['query']['pages'].values()))\n",
    "    wiki_text = page['extract']\n",
    "\n",
    "    data_path = Path('data')\n",
    "    if not data_path.exists():\n",
    "        Path.mkdir(data_path)\n",
    "\n",
    "    with open(data_path / f\"{title}.txt\", 'w', encoding=\"utf-8\") as fp:\n",
    "        fp.write(wiki_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7353f65d",
   "metadata": {},
   "source": [
    "##### This snippet loads all files into Document objects in data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b45c131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not import azure.core python package.\n"
     ]
    }
   ],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "city_docs = {}\n",
    "for wiki_title in wiki_titles:\n",
    "    city_docs[wiki_title] = SimpleDirectoryReader(input_files=[f\"data/{wiki_title}.txt\"]).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37129a2",
   "metadata": {},
   "source": [
    "#### Defining the Set of Indexes\n",
    "##### We will now define a set of indexes and graphs over your data. You can think of each index/graph as a lightweight structure that solves a distinct use case.\n",
    "\n",
    "##### We will first define a vector index over the documents of each city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "580d1bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTVectorStoreIndex, ServiceContext, StorageContext, LLMPredictor\n",
    "from langchain.llms.openai import OpenAIChat\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "# set service context\n",
    "llm_predictor_gpt4 = LLMPredictor(llm=OpenAIChat(temperature=0))#, model_name=\"gpt-4\"\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm_predictor=llm_predictor_gpt4, chunk_size_limit=1024\n",
    ")\n",
    "\n",
    "# Build city document index\n",
    "vector_indices = {}\n",
    "for wiki_title in wiki_titles:\n",
    "    storage_context = StorageContext.from_defaults()\n",
    "    # build vector index\n",
    "    vector_indices[wiki_title] = GPTVectorStoreIndex.from_documents(\n",
    "        city_docs[wiki_title], \n",
    "        service_context=service_context,\n",
    "        storage_context=storage_context,\n",
    "    )\n",
    "    # set id for vector index\n",
    "    vector_indices[wiki_title].index_struct.index_id = wiki_title\n",
    "    # persist to disk\n",
    "    storage_context.persist(persist_dir=f'./storage/{wiki_title}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb8863c",
   "metadata": {},
   "source": [
    "##### Querying a vector index lets us easily perform semantic search over a given city’s documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82ed18eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ff97ffe453839984d9ed83a1a80dd8b2 in your message.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ff97ffe453839984d9ed83a1a80dd8b2 in your message.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ff97ffe453839984d9ed83a1a80dd8b2 in your message.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 23 May 2023 13:19:26 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-processing-ms': '132', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3', 'x-ratelimit-limit-tokens': '40000', 'x-ratelimit-remaining-requests': '2', 'x-ratelimit-remaining-tokens': '37879', 'x-ratelimit-reset-requests': '20s', 'x-ratelimit-reset-tokens': '3.181s', 'x-request-id': 'ff97ffe453839984d9ed83a1a80dd8b2', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7cbd9729ebf09a83-NAG', 'alt-svc': 'h3=\":443\"; ma=86400, h3-29=\":443\"; ma=86400'}.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 4e1cbe860c95afa518ac0b178b02c053 in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 4e1cbe860c95afa518ac0b178b02c053 in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 4e1cbe860c95afa518ac0b178b02c053 in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 23 May 2023 13:19:30 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-organization': 'user-ec76pkpzu7sno7vdyio3ouwz', 'openai-processing-ms': '250', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3', 'x-ratelimit-limit-tokens': '40000', 'x-ratelimit-remaining-requests': '1', 'x-ratelimit-remaining-tokens': '37879', 'x-ratelimit-reset-requests': '35.536s', 'x-ratelimit-reset-tokens': '3.181s', 'x-request-id': '4e1cbe860c95afa518ac0b178b02c053', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7cbd97462c759a83-NAG', 'alt-svc': 'h3=\":443\"; ma=86400, h3-29=\":443\"; ma=86400'}.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b6a31ea51be2f9fa930e09cc42d4ee19 in your message.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b6a31ea51be2f9fa930e09cc42d4ee19 in your message.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b6a31ea51be2f9fa930e09cc42d4ee19 in your message.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 23 May 2023 13:19:35 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-processing-ms': '136', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3', 'x-ratelimit-limit-tokens': '40000', 'x-ratelimit-remaining-requests': '0', 'x-ratelimit-remaining-tokens': '37879', 'x-ratelimit-reset-requests': '50.94s', 'x-ratelimit-reset-tokens': '3.181s', 'x-request-id': 'b6a31ea51be2f9fa930e09cc42d4ee19', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7cbd9762cdc09a83-NAG', 'alt-svc': 'h3=\":443\"; ma=86400, h3-29=\":443\"; ma=86400'}.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toronto is represented in five major league sports, with teams in the National Hockey League (NHL), Major League Baseball (MLB), National Basketball Association (NBA), Canadian Football League (CFL), and Major League Soccer (MLS). The specific teams are the Toronto Maple Leafs, Toronto Blue Jays, Toronto Raptors, Toronto Argonauts, and Toronto FC.\n"
     ]
    }
   ],
   "source": [
    "query_engine = vector_indices[\"Toronto\"].as_query_engine()\n",
    "response = query_engine.query(\"What are the sports teams in Toronto?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fe3605",
   "metadata": {},
   "source": [
    "#### Defining a Graph for Compare/Contrast Queries\n",
    "##### We will now define a composed graph in order to run compare/contrast queries (see use cases doc). This graph contains a keyword table composed on top of existing vector indexes.\n",
    "\n",
    "##### To do this, we first want to set the “summary text” for each vector index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c933c6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_summaries = {}\n",
    "for wiki_title in wiki_titles:\n",
    "    # set summary text for city\n",
    "    index_summaries[wiki_title] = (\n",
    "        f\"This content contains Wikipedia articles about {wiki_title}. \"\n",
    "        f\"Use this index if you need to lookup specific facts about {wiki_title}.\\n\"\n",
    "        \"Do not use this index if you want to analyze multiple cities.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154ae37a",
   "metadata": {},
   "source": [
    "##### Next, we compose a keyword table on top of these vector indexes, with these indexes and summaries, in order to build the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a48f9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.indices.composability import ComposableGraph\n",
    "from llama_index import GPTSimpleKeywordTableIndex\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "graph = ComposableGraph.from_indices(\n",
    "    GPTSimpleKeywordTableIndex,\n",
    "    [index for _, index in vector_indices.items()], \n",
    "    [summary for _, summary in index_summaries.items()],\n",
    "    max_keywords_per_chunk=50\n",
    ")\n",
    "\n",
    "# get root index\n",
    "#root_index = graph.get_index( graph.index_struct.root_id,GPTSimpleKeywordTableIndex) #graph.index_struct.root_id,\n",
    "# set id of root index\n",
    "#root_index.set_index_id(\"compare_contrast\")\n",
    "root_summary = (\n",
    "    \"This index contains Wikipedia articles about multiple cities. \"\n",
    "    \"Use this index if you want to compare multiple cities. \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e00a6e",
   "metadata": {},
   "source": [
    "##### Querying this graph (with a query transform module), allows us to easily compare/contrast between different cities. An example is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0967f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m\u001b[1;3m> Current query: Compare and contrast the arts and culture of Houston and Boston. \n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What are some notable cultural institutions or events in Houston?\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m> Current query: Compare and contrast the arts and culture of Houston and Boston. \n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What are some notable cultural institutions or events in Houston?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 03e9bacfcb442c5545fcf354f38e85b3 in your message.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 03e9bacfcb442c5545fcf354f38e85b3 in your message.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 03e9bacfcb442c5545fcf354f38e85b3 in your message.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 23 May 2023 13:27:36 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-processing-ms': '116', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3', 'x-ratelimit-limit-tokens': '40000', 'x-ratelimit-remaining-requests': '0', 'x-ratelimit-remaining-tokens': '39649', 'x-ratelimit-reset-requests': '59.687s', 'x-ratelimit-reset-tokens': '526ms', 'x-request-id': '03e9bacfcb442c5545fcf354f38e85b3', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7cbda3259b7c854f-BOM', 'alt-svc': 'h3=\":443\"; ma=86400, h3-29=\":443\"; ma=86400'}.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\llama_index\\utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
      "    return lambda_fn()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\llama_index\\llm_predictor\\base.py\", line 211, in <lambda>\n",
      "    lambda: llm_chain.predict(**full_prompt_args),\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 213, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\", line 140, in __call__\n",
      "    raise e\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\", line 134, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 69, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 79, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 134, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 191, in generate\n",
      "    raise e\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 185, in generate\n",
      "    self._generate(prompts, stop=stop, run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 746, in _generate\n",
      "    full_response = completion_with_retry(self, messages=messages, **params)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 106, in completion_with_retry\n",
      "    return _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 104, in _completion_with_retry\n",
      "    return llm.client.create(**kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 230, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 624, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 687, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ba31833463a05c236ca71f6f44b40feb in your message.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ba31833463a05c236ca71f6f44b40feb in your message.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ba31833463a05c236ca71f6f44b40feb in your message.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 23 May 2023 13:27:56 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-processing-ms': '149', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3', 'x-ratelimit-limit-tokens': '40000', 'x-ratelimit-remaining-requests': '0', 'x-ratelimit-remaining-tokens': '39649', 'x-ratelimit-reset-requests': '59.806s', 'x-ratelimit-reset-tokens': '526ms', 'x-request-id': 'ba31833463a05c236ca71f6f44b40feb', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7cbda3a1ef0a854f-BOM', 'alt-svc': 'h3=\":443\"; ma=86400, h3-29=\":443\"; ma=86400'}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 5bad15b2afcf16b3da91456d23f49466 in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 5bad15b2afcf16b3da91456d23f49466 in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 5bad15b2afcf16b3da91456d23f49466 in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 23 May 2023 13:28:18 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-organization': 'user-ec76pkpzu7sno7vdyio3ouwz', 'openai-processing-ms': '291', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3', 'x-ratelimit-limit-tokens': '40000', 'x-ratelimit-remaining-requests': '0', 'x-ratelimit-remaining-tokens': '39649', 'x-ratelimit-reset-requests': '58.07s', 'x-ratelimit-reset-tokens': '526ms', 'x-request-id': '5bad15b2afcf16b3da91456d23f49466', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7cbda429cbca854f-BOM', 'alt-svc': 'h3=\":443\"; ma=86400, h3-29=\":443\"; ma=86400'}.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\llama_index\\utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
      "    return lambda_fn()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\llama_index\\llm_predictor\\base.py\", line 211, in <lambda>\n",
      "    lambda: llm_chain.predict(**full_prompt_args),\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 213, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\", line 140, in __call__\n",
      "    raise e\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\", line 134, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 69, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 79, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 134, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 191, in generate\n",
      "    raise e\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 185, in generate\n",
      "    self._generate(prompts, stop=stop, run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 746, in _generate\n",
      "    full_response = completion_with_retry(self, messages=messages, **params)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 106, in completion_with_retry\n",
      "    return _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 104, in _completion_with_retry\n",
      "    return llm.client.create(**kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 230, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 624, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 687, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 740804732399a0c51561bf836cec3d93 in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 740804732399a0c51561bf836cec3d93 in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 740804732399a0c51561bf836cec3d93 in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 23 May 2023 13:28:40 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-organization': 'user-ec76pkpzu7sno7vdyio3ouwz', 'openai-processing-ms': '240', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3', 'x-ratelimit-limit-tokens': '40000', 'x-ratelimit-remaining-requests': '0', 'x-ratelimit-remaining-tokens': '39649', 'x-ratelimit-reset-requests': '56.632s', 'x-ratelimit-reset-tokens': '526ms', 'x-request-id': '740804732399a0c51561bf836cec3d93', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7cbda4afba57854f-BOM', 'alt-svc': 'h3=\":443\"; ma=86400, h3-29=\":443\"; ma=86400'}.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m\u001b[1;3m> Current query: Compare and contrast the arts and culture of Houston and Boston. \n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What are some notable cultural institutions or events in Boston?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0b6487007abe6b285744fc8bdbd83575 in your message.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0b6487007abe6b285744fc8bdbd83575 in your message.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0b6487007abe6b285744fc8bdbd83575 in your message.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 23 May 2023 13:29:18 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-processing-ms': '146', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3', 'x-ratelimit-limit-tokens': '40000', 'x-ratelimit-remaining-requests': '0', 'x-ratelimit-remaining-tokens': '39649', 'x-ratelimit-reset-requests': '57.924s', 'x-ratelimit-reset-tokens': '526ms', 'x-request-id': '0b6487007abe6b285744fc8bdbd83575', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7cbda5a1ab95854f-BOM', 'alt-svc': 'h3=\":443\"; ma=86400, h3-29=\":443\"; ma=86400'}.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m\u001b[1;3m> Current query: Compare and contrast the arts and culture of Houston and Boston. \n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What are some notable cultural institutions or events in Boston?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d96ed0ad0b31028aad47daf2c044152d in your message.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d96ed0ad0b31028aad47daf2c044152d in your message.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d96ed0ad0b31028aad47daf2c044152d in your message.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 23 May 2023 13:30:04 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-processing-ms': '111', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3', 'x-ratelimit-limit-tokens': '40000', 'x-ratelimit-remaining-requests': '0', 'x-ratelimit-remaining-tokens': '37744', 'x-ratelimit-reset-requests': '51.69s', 'x-ratelimit-reset-tokens': '3.384s', 'x-request-id': 'd96ed0ad0b31028aad47daf2c044152d', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7cbda6c26be8854f-BOM', 'alt-svc': 'h3=\":443\"; ma=86400, h3-29=\":443\"; ma=86400'}.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\llama_index\\utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
      "    return lambda_fn()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\llama_index\\llm_predictor\\base.py\", line 211, in <lambda>\n",
      "    lambda: llm_chain.predict(**full_prompt_args),\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 213, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\", line 140, in __call__\n",
      "    raise e\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\", line 134, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 69, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 79, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 134, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 191, in generate\n",
      "    raise e\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 185, in generate\n",
      "    self._generate(prompts, stop=stop, run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 746, in _generate\n",
      "    full_response = completion_with_retry(self, messages=messages, **params)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 106, in completion_with_retry\n",
      "    return _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 104, in _completion_with_retry\n",
      "    return llm.client.create(**kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 230, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 624, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 687, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    }
   ],
   "source": [
    "# define decompose_transform\n",
    "from llama_index.indices.query.query_transform.base import DecomposeQueryTransform\n",
    "#from llama_index import llm_predictor_chatgpt\n",
    "#from langchain import LLMPredictor\n",
    "#,LLMPredictor,PromptHelper\n",
    "decompose_transform = DecomposeQueryTransform(\n",
    "    llm_predictor_gpt4, verbose=True\n",
    ")\n",
    "\n",
    "# define custom query engines\n",
    "from llama_index.query_engine.transform_query_engine import TransformQueryEngine\n",
    "custom_query_engines = {}\n",
    "for index in vector_indices.values():\n",
    "    query_engine = index.as_query_engine(service_context=service_context)\n",
    "    query_engine = TransformQueryEngine(\n",
    "        query_engine,\n",
    "        query_transform=decompose_transform,\n",
    "        transform_extra_info={'index_summary': index.index_struct.summary},\n",
    "    )\n",
    "    custom_query_engines[index.index_id] = query_engine\n",
    "custom_query_engines[graph.root_id] = graph.root_index.as_query_engine(\n",
    "    retriever_mode='simple',\n",
    "    response_mode='tree_summarize',\n",
    "    service_context=service_context,\n",
    ")\n",
    "\n",
    "# define query engine\n",
    "graph_query_engine = graph.as_query_engine(custom_query_engines=custom_query_engines)\n",
    "\n",
    "# query the graph\n",
    "query_str = (\n",
    "    \"Compare and contrast the arts and culture of Houston and Boston. \"\n",
    ")\n",
    "response_chatgpt = graph_query_engine.query(query_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d8e7f1",
   "metadata": {},
   "source": [
    "#### Defining the Unified Query Interface\n",
    "##### Now that we’ve defined the set of indexes/graphs, we want to build an outer abstraction layer that provides a unified query interface to our data structures. This means that during query-time, we can query this outer abstraction layer and trust that the right index/graph will be used for the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36f2021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools.query_engine import QueryEngineTool\n",
    "\n",
    "query_engine_tools = []\n",
    "\n",
    "# add vector index tools\n",
    "for wiki_title in wiki_titles:\n",
    "    index = vector_indices[wiki_title]\n",
    "    summary = index_summaries[wiki_title]\n",
    "    \n",
    "    query_engine = index.as_query_engine(service_context=service_context)\n",
    "    vector_tool = QueryEngineTool.from_defaults(query_engine, description=summary)\n",
    "    query_engine_tools.append(vector_tool)\n",
    "\n",
    "\n",
    "# add graph tool\n",
    "graph_description = (\n",
    "    \"This tool contains Wikipedia articles about multiple cities. \"\n",
    "    \"Use this tool if you want to compare multiple cities. \"\n",
    ")\n",
    "graph_tool = QueryEngineTool.from_defaults(graph_query_engine, description=graph_description)\n",
    "query_engine_tools.append(graph_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f0f0c3",
   "metadata": {},
   "source": [
    "##### Now, we can define the routing logic and overall router query engine. Here, we use the LLMSingleSelector, which uses LLM to choose a underlying query engine to route the query to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e138d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.selectors.llm_selectors import LLMSingleSelector\n",
    "\n",
    "\n",
    "router_query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(service_context=service_context),\n",
    "    query_engine_tools=query_engine_tools\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b339d2cd",
   "metadata": {},
   "source": [
    "##### Asking a Compare/Contrast Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44b42537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 62037020ba6078982e4db02939754105 in your message.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 62037020ba6078982e4db02939754105 in your message.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 62037020ba6078982e4db02939754105 in your message.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 23 May 2023 13:31:27 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-processing-ms': '158', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3', 'x-ratelimit-limit-tokens': '40000', 'x-ratelimit-remaining-requests': '1', 'x-ratelimit-remaining-tokens': '39527', 'x-ratelimit-reset-requests': '29.022s', 'x-ratelimit-reset-tokens': '709ms', 'x-request-id': '62037020ba6078982e4db02939754105', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7cbda8c7496d854f-BOM', 'alt-svc': 'h3=\":443\"; ma=86400, h3-29=\":443\"; ma=86400'}.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m\u001b[1;3m> Current query: Compare and contrast the arts and culture of Houston and Boston.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What are some notable cultural institutions or events in Houston?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 9a993b8e380fbccaf1f2ca43a952196e in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 9a993b8e380fbccaf1f2ca43a952196e in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 9a993b8e380fbccaf1f2ca43a952196e in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 23 May 2023 13:32:01 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-organization': 'user-ec76pkpzu7sno7vdyio3ouwz', 'openai-processing-ms': '232', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3', 'x-ratelimit-limit-tokens': '40000', 'x-ratelimit-remaining-requests': '0', 'x-ratelimit-remaining-tokens': '39649', 'x-ratelimit-reset-requests': '54.87s', 'x-ratelimit-reset-tokens': '526ms', 'x-request-id': '9a993b8e380fbccaf1f2ca43a952196e', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7cbda99ccac7854f-BOM', 'alt-svc': 'h3=\":443\"; ma=86400, h3-29=\":443\"; ma=86400'}.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\llama_index\\utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
      "    return lambda_fn()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\llama_index\\llm_predictor\\base.py\", line 211, in <lambda>\n",
      "    lambda: llm_chain.predict(**full_prompt_args),\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 213, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\", line 140, in __call__\n",
      "    raise e\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\", line 134, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 69, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 79, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 134, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 191, in generate\n",
      "    raise e\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 185, in generate\n",
      "    self._generate(prompts, stop=stop, run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 746, in _generate\n",
      "    full_response = completion_with_retry(self, messages=messages, **params)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 106, in completion_with_retry\n",
      "    return _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 104, in _completion_with_retry\n",
      "    return llm.client.create(**kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 230, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 624, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 687, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 989ae34c88ea0b23080df468975fce51 in your message.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 989ae34c88ea0b23080df468975fce51 in your message.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 989ae34c88ea0b23080df468975fce51 in your message.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 23 May 2023 13:32:17 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-processing-ms': '121', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3', 'x-ratelimit-limit-tokens': '40000', 'x-ratelimit-remaining-requests': '0', 'x-ratelimit-remaining-tokens': '39649', 'x-ratelimit-reset-requests': '58.953s', 'x-ratelimit-reset-tokens': '526ms', 'x-request-id': '989ae34c88ea0b23080df468975fce51', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7cbdaa004e7d854f-BOM', 'alt-svc': 'h3=\":443\"; ma=86400, h3-29=\":443\"; ma=86400'}.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m\u001b[1;3m> Current query: Compare and contrast the arts and culture of Houston and Boston.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What are some notable cultural institutions or events in Houston?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID bf32f1cfa4934050fb9bfe07ecdbeafa in your message.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID bf32f1cfa4934050fb9bfe07ecdbeafa in your message.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID bf32f1cfa4934050fb9bfe07ecdbeafa in your message.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 23 May 2023 13:33:24 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-processing-ms': '110', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3', 'x-ratelimit-limit-tokens': '40000', 'x-ratelimit-remaining-requests': '0', 'x-ratelimit-remaining-tokens': '39649', 'x-ratelimit-reset-requests': '51.85s', 'x-ratelimit-reset-tokens': '526ms', 'x-request-id': 'bf32f1cfa4934050fb9bfe07ecdbeafa', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7cbdaba3a916854f-BOM', 'alt-svc': 'h3=\":443\"; ma=86400, h3-29=\":443\"; ma=86400'}.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\llama_index\\utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
      "    return lambda_fn()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\llama_index\\llm_predictor\\base.py\", line 211, in <lambda>\n",
      "    lambda: llm_chain.predict(**full_prompt_args),\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 213, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\", line 140, in __call__\n",
      "    raise e\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\", line 134, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 69, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 79, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 134, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 191, in generate\n",
      "    raise e\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 185, in generate\n",
      "    self._generate(prompts, stop=stop, run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 746, in _generate\n",
      "    full_response = completion_with_retry(self, messages=messages, **params)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 106, in completion_with_retry\n",
      "    return _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 104, in _completion_with_retry\n",
      "    return llm.client.create(**kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 230, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 624, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 687, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m\u001b[1;3m> Current query: Compare and contrast the arts and culture of Houston and Boston.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What are some notable cultural institutions or events in Boston?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID ef25d6ea5b705ee94b72c8a827377dc9 in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID ef25d6ea5b705ee94b72c8a827377dc9 in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID ef25d6ea5b705ee94b72c8a827377dc9 in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 23 May 2023 13:34:04 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-organization': 'user-ec76pkpzu7sno7vdyio3ouwz', 'openai-processing-ms': '227', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3', 'x-ratelimit-limit-tokens': '40000', 'x-ratelimit-remaining-requests': '0', 'x-ratelimit-remaining-tokens': '39649', 'x-ratelimit-reset-requests': '52.513s', 'x-ratelimit-reset-tokens': '526ms', 'x-request-id': 'ef25d6ea5b705ee94b72c8a827377dc9', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7cbdac998a4a854f-BOM', 'alt-svc': 'h3=\":443\"; ma=86400, h3-29=\":443\"; ma=86400'}.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\llama_index\\utils.py\", line 177, in retry_on_exceptions_with_backoff\n",
      "    return lambda_fn()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\llama_index\\llm_predictor\\base.py\", line 211, in <lambda>\n",
      "    lambda: llm_chain.predict(**full_prompt_args),\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 213, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\", line 140, in __call__\n",
      "    raise e\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py\", line 134, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 69, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py\", line 79, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 134, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 191, in generate\n",
      "    raise e\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py\", line 185, in generate\n",
      "    self._generate(prompts, stop=stop, run_manager=run_manager)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 746, in _generate\n",
      "    full_response = completion_with_retry(self, messages=messages, **params)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 106, in completion_with_retry\n",
      "    return _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\tenacity\\__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\langchain\\llms\\openai.py\", line 104, in _completion_with_retry\n",
      "    return llm.client.create(**kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 230, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 624, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 687, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m\u001b[1;3m> Current query: Compare and contrast the arts and culture of Houston and Boston.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What are some notable cultural institutions or events in Boston?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-19pT0RCfHf630cjmlNbEmSjr on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Houston and Boston both have a rich arts and culture scene with a variety of institutions and events. In Houston, notable events include the Houston Livestock Show and Rodeo, the Houston Greek Festival, and the Art Car Parade, while Boston has the Boston Arts Festival, Italian summer feasts in the North End, and the Fourth of July Harborfest festivities and Boston Pops concert. Both cities have museums of fine arts and natural science, as well as contemporary art museums. Houston also has a Holocaust Museum and a Space Center, while Boston has the Isabella Stewart Gardner Museum and the Boston Early Music Festival. Both cities also have gay pride parades and festivals. Overall, while there are some differences in specific events and institutions, both Houston and Boston offer a diverse and vibrant arts and culture scene.\n"
     ]
    }
   ],
   "source": [
    "# ask a compare/contrast question \n",
    "response = router_query_engine.query(\n",
    "    \"Compare and contrast the arts and culture of Houston and Boston.\",\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b280aab",
   "metadata": {},
   "source": [
    "##### Asking Questions about specific Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c100c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = router_query_engine.query(\"What are the sports teams in Toronto?\")\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
